softmax operation = 모든 값에 expo를 하고 곱함/ 모든 값을 expo해서 더함

One hot encoding이란?
원 -핫 인코딩은 컴퓨터가 문자를 숫자로 바꾸어 처리하는 기법 중 하나이다.
-핵심 : 표현하고 싶은 단어의 인덱스에 1을 주고, 다른 인덱스들에는 0을 부여하는 
벡터적 표현 방식이다.

target 값에 대해
내가 원하는 값만 1이고 나머지는 0으로 처리한 coding

Cross - entropy란?
- 딥러닝에서 cost function으로 사용하는 방법인데 수렴의 속도가 빨라서 좋다
cross-entropy를 알기 위해서는
1) information - 놀람의 정보 즉, 정보량 h(x) = -logP(x)로 선언
P(x)는 확률이다.

2) entropy는 이런 정보량의 평균

즉 확률변수의 평균 정보량 = 놀람의 평균적인 정도 = 불확실성의 정도
entropy = E(-log(Q(x))) - E(-log(P(x)))에서 E(-log(Q(x)))를 minimize해야한다.

이때의 E(-log(Q(x)))를 cross entropy라고 부른다.


3) KL - divergence = 두 entropy의 거리 = 상대적 entropy = 두 entropy의 차이
이제 KL-divergence를 minimize하려면 E

cross entropy function의 값을 줄이겠다는 것은 내가 갖고 있는 값을 제외한
나머지 index의 값은 고려하지 않는다.
즉 내 타겟값만 1로 두고 나머지는 0으로 두겠다.

class 100개있는데 3번째 인덱스 속한다고 가정 
->이는 내가 가진 클래스가 3번째 class 이므로
 3번째 클래스의 숫자만 높이고
나머지 99개 클래스는 신경쓰지 않겠다.
이것이 cross entropy



----------
cross entropy와 squared loss 함수의 차이

분류 문제 -> cross entropy가 효과적


----------------------
용어 정리

epoch와 batch, iteration

1. epoch = 1epoch는 인공신경망에서 전체 데이터 셋에 대해 foward pass/ backward pass 과정을
거친 것을 말한다.
즉 , 전체 데이터 셋에 대해 한번의 학습만을 완료한 상태

ex) epoch가 40이면 forward backward pass를 통해 데이터셋 전체를 40번 학습 시킴
epoch를 적절히 = 작으면 underfittin가능, 크면 overfitting 발생 가능

2. batch
bacth 사이즈는 한번의 batch마다 내가 임의로 설정한 data sample의 size이다.
mini batch라는 표현은  나눠진 데이터 셋을 의미

3. iteration은 epoch를 분할하여 실행하는 횟수이다.

memory의 한계와 속도를 고려하여 이렇게 epoch를 batchsize를 정하여 
epoch를 iteration단위로 나누어서 실행한다.





