-전이학습 summary-

목표 - 전이 학습을 어떻게 이용하는지 배운다.
keyward = 전이학습(transfer learning), 동결(freeze)

딥러닝 학습을 시작할 때는 무작위 랜덤의 과정으로 시작하는 것보다 다른이들이 훈련 시켜놓은 구조를 다운 받는 것이
훨씬 효과적이다.

모든 층의 변수들을 고정 시키는것 = freeze (동결)

소프트맥스 층과 관련된 변수만 훈련 시킨다.
trainable parameter = 0으로 설정하면 이부분의 변수에 대해서는 훈련시키지 않게 된다.
또는 freeze = 1
이것이 딥러닝 프레임워크가 훈련 을 시키는지 결정
이전층들을 freeze 시킨다.  훈련의 속도를 높이는 방법은 input 이미지를 특정 활성값으로 전달하는 그 활성층을
미리 계산하는 것(디스크에 미리 저장)
이 고정함수를 가지고 어떤 이미지x를 입력받아 계산후 소프트맥스 모델을 훈련한다. 미리 활성값을 계산한다.
모든 훈련 세트의 활성값을 계속 저장하면 softmax만 훈련하면 된다.
미리 계산하고 디스크에 활성값을 저장하게 되면 속도가 빠르다. (훈련 세트가 작을 때에 한해)

많은 훈련세트 일때는 ??
몇개의 층만 freeze 시킨다 전부 말고
그리고 그 이후의 층들을 훈련시킨다.  마지막 몇개의 층을 선택해서 초기화로 활용후 경사하강법 사용 또는
뒤쪽층을 버리고 새로운 히든층을 만들고 소프트맥스 출력을 해도 됨
데이터가 많을 수록 동결 시키는 층을 줄이고 훈련시킬 층의 갯수를 늘려야 한다.

더 큰 데이터 세트라면 하나의 소프트맥스 유닛만을 훈련시키지 않고 동결된 층 이후의 마지막 몇개의 층을
조합해서 신경망 안의 작은 신경망을 만들어 훈련 할 수 있다.

방대한 데이터라면 오픈 소스 네트워크와 가중치를 받아서 초기화과정으로 다 네트워크 전체를 다시 훈련한다.
수많은 이미지 input이라면 그만큼 소프트맥스 출력 클래스도 많기에 내가 필요한 클래스의 유닛수로 줄이면 된다.

오픈소스 가중치를 초기값으로 사용하면 굉장히 효율적이고 유용하다. 
전이학습은 중요하다. 왠만한 데이터 집합을 학습시킬 때 -> 오픈소스의 가중치와 네트워크를 받아와서
일정부분 까지의 layer를 동결(변수들을 고정시키고) 그 뒤부터 마지막 layer까지를 훈련시켜서 softmax를 출력하면
효율적으로 신경망을 학습시킬 수 있고 시간과 비용을 절약할 수 있다.

남이 먼저 해놓은 것을 토대로 가중치와 네트워크를 참조하면 더 빨리 학습을 가능케 할 수 있는게
전이학습의 장점이자 특징이고 사용하는 이유이다.

학습내용-
미리 훈련된 모델로 풀고자 하는 문제의 초기값을 사용할 수 있다. 즉, 전이학습을 통해 공유되는 지식을 전달하는 것
전이학습(미리 훈련된 모델을 내가 풀려는 문제의 모델에 전이 시켜서 써먹는것)
전이학습할 때 오픈소스에서 네트워크 모델 뿐만아니라 가중치도 함께 받아야한다.

내가 풀려는 문제의 데이터가 적으면 전이시킨 (즉 미리 훈련된 모델의 최종 분류층인 활성화층(소프트맥스 층)을 제거하고
내것에 맞게 분류층(소프트맥스층)을 설정한다(일종의 커스터마이즈) 그리고 모델의 나머지 부분은 학습을 시키지 않게
하기 위해 동결(freeze)시키고 마지막 최종층 (활성화층)만 훈련 시킨다.
데이터가 많을 수록 동결시키는 층은 줄어들고 훈련시킬 층의 갯수가 늘어난다.
